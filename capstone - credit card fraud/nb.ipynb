{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- ##### Load data/Data understanding\n",
    "    > Here, you need to load the data and understand the features present in it. This would help you choose the features that you will need for your final model.\n",
    "- ##### select features\n",
    "    > select features other than pcs components if necessary and create a subset\n",
    "- ##### EDA\n",
    "    - since all are gaussian, no need to do z-scaling\n",
    "    > Usually, in this step, you need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, you do not need to perform Z-scaling. However, you can check if there is any skewness in the data and try to mitigate it, as it might cause problems during the model building phase.\n",
    "    \n",
    "    - check for skewness \n",
    "        - mitigate skewness if present\n",
    "\n",
    "    > Can you think of the reason why skewness can be an issue while modelling? Well, some of the data points in a skewed distribution towards the tail may act as outliers for the machine learning models that are sensitive to outliers; hence, this may cause a problem. Also, if the values of any independent feature are skewed, depending on the model, __skewness__ may affect model assumptions or may impair the interpretation of feature importance.\n",
    "- ##### Train - Test split\n",
    "    - K - fold cross validation\n",
    "        - k value need to taken inelligently as the target variable is highly imbalance - only 500 out the toal are fraud cases. we will hardly have 50 entries in test fold if __k=10__. So if k is less then the accuracy might not be good\n",
    "    > Now, you are familiar with the train/test split that you can perform to check the performance of your models with unseen data. Here, for validation, you can use the k-fold cross-validation method. You need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n",
    "- ##### Model building/Hyper params tuning\n",
    "    > This is the final step at which you can try different models and fine-tune their hyperparameters until you get the desired level of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* undersampling \n",
    "    * using only 500 0's ans 500 1's for balancing imblanace\n",
    "* Randomly/Uniformly oversampling minor class \n",
    "    * repeating some of the minority class entries but this will only exagarate the data\n",
    "* SMOTE: Synthetic Minority Over-Sampling Technique\n",
    "    * Generate new data points that lie vectorially between two data points that belong to the minority class. These data points are randomly chosen and then assigned to the minority class. This method uses the K-nearest neighbours to create random synthetic samples\n",
    "* ADAptive SYNthetic (ADASYN)\n",
    "    *  This is similar to SMOTE, with a minor change in the generation of synthetic sample points for minority data points. For a particular data point, the number of synthetic samples that it will add will have a density distribution, whereas for SMOTE, the distribution will be uniform. The aim here is to create synthetic data for minority examples that are harder to learn rather than easier ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To sum it up, ADASYN offers the following advantages:\n",
    "\n",
    "    - It lowers the bias introduced by the class imbalance.\n",
    "    - It adaptively shifts the classification decision boundary towards difficult examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample:\\n\\nfrom imblearn.over_sampling import SMOTE, ADASYN\\nX_smote, y_amote = SMOTE().fit_resample(X, y)\\nX_ada, y_ada = ADASYN().fit_resample(X, y)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "X_smote, y_amote = SMOTE().fit_resample(X, y)\n",
    "X_ada, y_ada = ADASYN().fit_resample(X, y)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-nearest neighbour is a simple, supervised machine learning algorithm that is used for both classification and regression tasks. It performs these tasks by identifying the neighbours that are the nearest to a data point. For classification tasks, it takes the majority vote and for regression tasks it takes the average value from the neighbours. \n",
    "\n",
    "* The k in KNN specifies the number of neighbours that the algorithm should focus on. For example, if k = 3 then for a particular test data, the algorithm observes the three nearest neighbours and takes the majority vote from them. Depending on the majority of the classes from the three nearby points, the algorithm classifies the test data.\n",
    "\n",
    "* the k value should be an odd number because you have to take the majority vote from the nearest neighbours by breaking the ties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if k is very low -> k = 1 -> overfitting\n",
    "if k is very high -> k = 101 -> underfitting\n",
    "\n",
    "good range: k = 3 to 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">K-NN doc reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* eXtreme Gradient Boosting \n",
    "    * It is a decision tree-based ensemble ML algorithm that uses a gradient boosting framework. It is a highly effective and widely used machine learning method and has applications for structured and unstructured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic Regression\n",
    "    * Pros:\n",
    "        - only works on Linearly separable data\n",
    "        - Highly interpretable\n",
    "    * Cons:\n",
    "        - Fails if data overlaps\n",
    "* K-NN\n",
    "    * Pros:\n",
    "        - Very interpretable\n",
    "    * Cons:\n",
    "        - computing is very high for large data: To find even one neighbour of a test point, we need to calculate distance from all th epoints in the dataset\n",
    "* Decision Trees:\n",
    "    * Pros:\n",
    "        - Interpretation interms of flow charts\n",
    "    * Cons:\n",
    "        - Trees tend to overfit\n",
    "        - working with large data becomes challenging because quadratic computing requires a lot of training time for large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the data is structured -> use Random Forests / XGBoost\n",
    "- If the data is unstructured -> use Nerual Networks / LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "startified k-fold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search cv and randomized search cv for hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy is not always good metrics\n",
    "* Confusion matrix is not always best metric as it depends on threshold\n",
    "* same goes with Precision, Recall, and F1 score\n",
    "* AUC-ROC curve -> TPR vs FPR\n",
    "    - Better the ROC curve, better the model\n",
    "    - The more the Area under the curve, better the model\n",
    "    - the best threshold would be one at which the TPR is high and FPR is low, i.e., misclassifications are low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "593d114b5a37efc49ac6bd5496f0496027154ecfb36472b8cce6beb026f23cc8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
